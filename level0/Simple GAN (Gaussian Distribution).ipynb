{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generative Adversarial Networks\n",
    "\n",
    "Generative Andverarsarial Networks, [introduced](https://arxiv.org/abs/1406.2661) by Ian Goodfellow in 2014 have been the most powerfull model for generating any types from data from images ot text.\n",
    "\n",
    "This notebook will teach you the basics of generative adversarial models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import theano\n",
    "import theano.tensor as T\n",
    "\n",
    "import lasagne\n",
    "from lasagne.layers import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1) Define our Generator\n",
    "\n",
    "GANs consist of two models a generator, which generates data samples and a discriminator, which distinguishes samples generated by the generator from the original samples from the dataset.\n",
    "\n",
    "The generator learns the mapping between a latent space and a given data distripution. As an input it must recieve some kind of noise, it could be just a random distribution (in our tutorial we will use uniform noise) or it could be a condition (onehot vector, image or sentence fed through a dropout layer).  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#you can play around with the hyperparams as you want\n",
    "\n",
    "class generator:\n",
    "    input_noise = T.matrix(\"Noise\")\n",
    "    input_shape = [None, 32]\n",
    "    \n",
    "    inp = InputLayer(input_shape, input_noise)\n",
    "    hid = DenseLayer(inp, 128)\n",
    "    out = <no nonlinearity, shape of your choice>\n",
    "    \n",
    "    fake_gaussian = get_output(out)\n",
    "    \n",
    "    weights = get_all_params(out, trainable=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.2) Define our Discriminator\n",
    "\n",
    "The discriminator distinguishes generated samples from real ones by giving the labels 0, meaning the given sample is fake, and 1 meaning the opposite."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "real_gaussian = T.matrix(\"Target gaussian\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class discriminator:\n",
    "    inp = InputLayer(generator.out.output_shape)\n",
    "    hid = DenseLayer(inp, 64)\n",
    "    prob = <probabilities of sample being real>\n",
    "    \n",
    "    prob_fake = get_output(prob, {inp : generator.fake_gaussian})\n",
    "    prob_real = get_output(prob, {inp : real_gaussian})\n",
    "    \n",
    "    weights = get_all_params(prob)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2) Define our loss functions \n",
    "\n",
    "We train the generator to minimize $$-\\log(D(G(z)))$$\n",
    "\n",
    "And we train the discriminator to minimize $$-(\\log(x) + \\log(1 - G(z))$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "d_loss = -(T.log(discriminator.prob_real) + T.log(1 - discriminator.prob_fake)).mean()\n",
    "g_loss = -(T.log(discriminator.prob_fake)).mean()\n",
    "\n",
    "d_updates = lasagne.updates.adam(d_loss, discriminator.weights)\n",
    "g_updates = lasagne.updates.adam(g_loss, generator.weights)\n",
    "\n",
    "d_train = theano.function([generator.input_noise, real_gaussian], d_loss, updates=d_updates,\n",
    "                          allow_input_downcast=True)\n",
    "g_train = theano.function([generator.input_noise], g_loss, updates=g_updates, allow_input_downcast=True)\n",
    "\n",
    "sample = theano.function([generator.input_noise], generator.fake_gaussian, allow_input_downcast=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## A little helper function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def sample_batch(batch_size):\n",
    "    x = np.random.uniform(-10, 10, size=[batch_size] + list(generator.input_shape[1:]))\n",
    "    y = np.random.normal(size=[batch_size] + list(generator.out.output_shape[1:]))    \n",
    "    \n",
    "    return x, y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3) Training loop\n",
    "    1) Train discriminator\n",
    "    2) Train generator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_epochs = 2000\n",
    "batch_size = 32\n",
    "\n",
    "for ep in range(n_epochs):\n",
    "    x, y = sample_batch(batch_size)\n",
    "    \n",
    "    d_train(x, y)\n",
    "    \n",
    "    g_train(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "x, y = sample_batch(1)\n",
    "plt.hist(sample(x)[0])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
